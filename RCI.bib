% Encoding: UTF-8

@Book{barber:2012:book,
  author    = {David Barber},
  title     = {Bayesian Reasoning and Machine Learning},
  year      = {2012},
  publisher = {Cambridge University Press},
  abstract  = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
  address   = {New York, US},
  file      = {:Barber_2012_BayesianReasoningAndMachineLearning.pdf:PDF},
  keywords  = {bayesian reasoning, machine learning, probabilistic reasoning, belief networks, junction tree, decision making, nearest neighbor, dimensionality reduction, gaussian processes, mixture models, markov models},
}

@Book{bishop:2006:book,
  author    = {Christopher M. Bishop},
  title     = {Pattern Recognition and Machine Learning},
  year      = {2006},
  publisher = {Springer Science+Business Media},
  file      = {:Bishop_2006_PatternRecognitionAndMachineLearning.pdf:PDF},
  keywords  = {Pattern Recognition, Machine Learning, Neural Networks, Linear Classifiers, Linear Regression},
}

@Book{boyd:2004:book,
  author    = {Boyd, Stephen and Vandenberghe, Lieven},
  title     = {Convex Optimization},
  year      = {2004},
  publisher = {Cambridge University Press},
  address   = {New York, US},
  file      = {:Boyd_2004_ConvexOptimization.pdf:PDF},
  keywords  = {convex optimization},
}

@Book{ertel:2016:book,
  author    = {Wolfgang Ertel},
  title     = {Grundkurs Künstliche Intelligenz},
  year      = {2016},
  edition   = {4},
  publisher = {Springer Vieweg},
  abstract  = {Alle Teilgebiete der KI werden mit dieser Einführung kompakt, leicht verständlich und anwendungsbezogen dargestellt. Hier schreibt jemand, der das Gebiet nicht nur bestens aus Forschung und praktischer Anwendung kennt, sondern auch in der Lehre engagiert und erfolgreich vertritt. Von der klassischen Logik über das Schließen mit Unsicherheit und maschinelles Lernen bis hin zu Anwendungen wie Expertensysteme oder lernfähige Roboter.
Sie profitieren von dem umfassenden Einblick in dieses faszinierende Teilgebiet der Informatik, wobei abgesehen von grundlegenden Programmierkenntnissen sowie etwas Mathematik alle Voraussetzungen für ein gutes Verständnis bereitgestellt werden. Sie gewinnen vertiefte Kenntnisse, z. B. hinsichtlich der wichtigsten Verfahren zur Repräsentation und Verarbeitung von Wissen und in dem immer wichtiger werdenden Gebiet des maschinellen Lernens. Vor allem der Anwendungsbezug steht im Fokus der Darstellung. Viele Übungsaufgaben mit Lösungen sowie eine strukturierte Liste mit Verweisen auf Literatur und Ressourcen im Web ermöglichen ein effektives und kurzweiliges Selbststudium.},
  address   = {Wiesbaden, DE},
  file      = {:Ertel_2016_GrundkursKuenstlicheIntelligenz.pdf:PDF},
  keywords  = {autonomous agents, machine learning, bayesian networks, data mining, reinforcement learning, system diagnostics, neural networks, deep learning, inference, uncertainty, artificial intelligence},
}

@Book{gelman:2014:book,
  author    = {Andrew Gelman and John B. Carlin and Hal S. Stern and David B. Dunson and Aki Vehtari and Donald B. Rubin},
  title     = {Bayesian Data Analysis},
  year      = {2014},
  edition   = {3},
  publisher = {CRC Press},
  file      = {:Gelman_2014_BayesianDataModel.pdf:PDF},
  keywords  = {statistics, probability, Bayesian inference, regression models},
}

@Article{george:2017:art,
  author    = {George, D. and Lehrach, W. and Kansky, K. and L{\'a}zaro-Gredilla, M. and Laan, C. and Marthi, B. and Lou, X. and Meng, Z. and Liu, Y. and Wang, H. and Lavin, A. and Phoenix, D. S.},
  title     = {A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs},
  journal   = {Science},
  year      = {2017},
  month     = oct,
  issn      = {0036-8075},
  doi       = {10.1126/science.aag2612},
  eprint    = {http://science.sciencemag.org/content/early/2017/10/26/science.aag2612.full.pdf},
  url       = {http://science.sciencemag.org/content/early/2017/10/26/science.aag2612},
  abstract  = {Learning from few examples and generalizing to dramatically different situations are capabilities of human visual intelligence that are yet to be matched by leading machine learning models. By drawing inspiration from systems neuroscience, we introduce a probabilistic generative model for vision in which message-passing based inference handles recognition, segmentation and reasoning in a unified way. The model demonstrates excellent generalization and occlusion-reasoning capabilities, and outperforms deep neural networks on a challenging scene text recognition benchmark while being 300-fold more data efficient. In addition, the model fundamentally breaks the defense of modern text-based CAPTCHAs by generatively segmenting characters without CAPTCHA-specific heuristics. Our model emphasizes aspects like data efficiency and compositionality that may be important in the path toward general artificial intelligence.},
  file      = {:George_2017_GenerativeVisionModel.pdf:PDF},
  keywords  = {machine learning, generative model, CAPTCHA, character recognition, neuroscience, recursive cortical network},
  publisher = {American Association for the Advancement of Science},
}

@Book{goodfellow:2016:book,
  Title                    = {Deep Learning},
  Author                   = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  Publisher                = {MIT Press},
  Year                     = {2016},

  Keywords                 = {convolutional neural networt, machine learning, linear algebra, probability theory},
  Url                      = {http://www.deeplearningbook.org}
}

@Article{kumar:2018:art,
  author   = {Varun Ravi Kumar and Stefan Milz and Martin Simon and Christian Witt and Karl Amende and Johannes Petzold and Senthil Yogamani},
  title    = {Monocular Fisheye Camera Depth Estimation Using Semi-supervised Sparse Velodyne Data},
  year     = {2018},
  month    = {3},
  pages    = {15},
  url      = {https://arxiv.org/abs/1803.06192v2},
  abstract = {Near field depth estimation around a self driving car is an important function that can be achieved by four wide angle fisheye cameras having a field of view of over 180. CNN based depth estimation produce state of the art results, but progress is hindered because depth annotation cannot be obtained manually. Synthetic datasets are commonly used but they have limitations. For instance, they do not capture the extensive variability in the appearance of objects like vehicles present in real datasets. There is also a domain shift while performing inference on natural images illustrated by many attempts to handle the domain adaptation explicitly. In this work, we explore an alternate approach of training using sparse LIDAR data as ground truth for depth estimation for fisheye camera. We built our own dataset using our self\hyp driving car setup which has a 64 beam Velodyne LIDAR and four wide angle fisheye cameras. LIDAR data projected onto the image plane is sparse and hence viewed as semi supervision for dense depth estimation. To handle the difference in view points of LIDAR and fisheye camera, an occlusion resolution mechanism was implemented. We started with Eigen's multiscale convolutional network architecture and improved by modifying activation function and optimizer. We obtained promising results on our dataset with RMSE errors better than the state of the art results obtained on KITTI because of vast amounts of training data. Our model runs at 20 fps on an embedded platform Nvidia TX2.},
  file     = {:Kumar_2018_MonocularFisheyeDepthEstimation.pdf:PDF},
  keywords = {depth estimation, semi-supervised learning, fisheye camera, deep learning, lidar},
}

@Book{mackay:2003:book,
  author    = {MacKay, David J. C.},
  title     = {Information Theory, Inference \& Learning Algorithms},
  year      = {2003},
  publisher = {Cambridge University Press},
  address   = {New York, US},
  file      = {:MacKay_2003_InformationTheoryInferenceAndLearningAlgorithms.pdf:PDF},
  keywords  = {data compression, neural networks, noisy channel coding, sparse graph codes},
}

@Book{magnus:2007:book,
  author    = {Jan R. Magnus and Heinz Neudecker},
  title     = {Matrix Differential Calculus with Applications in Statistics and Econometrics},
  year      = {2007},
  edition   = {3},
  publisher = {John Wiley},
  abstract  = {This book provides a self-contained and unified treatment of matrix differential calculus, aimed at econometricians and statisticians. It can be used as a textbook for senior undergraduate or graduate courses on the subject, and will also be a valuable source for professional econometricians and statisticians who want to learn and apply these important techniques. The authors base their approach on differentials rather than derivatives, and they show that the use of differentials is elegant, easy and considerably more useful in applications. No specialist knowledge of matrix algebra or calculus is required, since the basics of matrix algebra are covered in the first three chapters with a thorough treatment of multivariable calculus provided in Chapters Four to Seven. Exercises are included in each chapter, and many examples which illustrate applications of the theory are considered in detail.},
  address   = {Chichester, UK},
  file      = {:Magnus_2007_DifferentialMatrixCalculus.pdf:PDF},
  keywords  = {differential matrix calculus, economics, linear algebra, statistics, exonometrics},
}

@Book{murphy:2012:book,
  author    = {Kevin P. Murphy},
  title     = {Machine Learning},
  year      = {2012},
  publisher = {MIT Press},
  file      = {:Murphy_2012_MachineLearning.pdf:PDF},
  keywords  = {machine learning, probability, generative models, Bayesian statistics, Bayes nets, sparse linear models, hidden Markov models, variational inference, Markov chain Monte Carlo inference, deep learning},
}

@Article{ruder:2016:art,
  author   = {Sebastian Ruder},
  title    = {An overview of gradient descent optimization algorithms},
  journal  = {CoRR},
  year     = {2016},
  volume   = {abs/1609.04747},
  url      = {http://arxiv.org/abs/1609.04747},
  file     = {:Ruder_2016_GradientDescentOptimization.pdf:PDF},
  keywords = {gradient descent, optimization, momentum, nesterov, Adagrad, Adadelta, RMSprop, Adam, AdaMax, Nadam},
}

@Book{russel:2009:book,
  author    = {Russell, Stuart and Norvig, Peter},
  title     = {Artificial Intelligence: A Modern Approach},
  date      = {2010},
  edition   = {3},
  publisher = {Prentice Hall Press},
  location  = {Upper Saddle River, US},
  abstract  = {The long-anticipated revision of this \#1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence.},
  file      = {:Russel_2010_ArtificialIntelligenceAModernApproach.pdf:PDF},
  keywords  = {artificial intelligence, intelligent agent, search methods, logical reasoning, knowledge base, first order logic, logical reasoning, probabilistic reasoning, uncertainty modelling, decision making, machine learning, reinforcement learning, robotics},
}

@Book{scholkopft:2001:book,
  Title                    = {Learning with Kernels - Support Vector Machines, Regularization, Optimization, and Beyond},
  Author                   = {Schölkopf, Bernhard and Smola, Alexander J.},
  Publisher                = {MIT Press},
  Year                     = {2001},

  Address                  = {Cambridge, US},

  Abstract                 = {In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs-kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.},
  Keywords                 = {support vector machine, kernel function, machine learning, regularization, optimization}
}

@Article{simon:2018:art,
  author   = {Martin Simon and Stefan Milz and Karl Amende and Horst-Michael Gross},
  title    = {Complex-YOLO: An Euler-Region-Proposal for Real-time 3D Object Detection on Point Clouds},
  year     = {2018},
  month    = {3},
  pages    = {14},
  url      = {https://arxiv.org/abs/1803.06199v1},
  abstract = {Lidar based 3D object detection is inevitable for autonomous driving, because it directly links to environmental understanding and therefore builds the base for prediction and motion planning. The capacity of inferencing highly sparse 3D data in real-time is an ill-posed problem for lots of other application areas besides automated vehicles, e.g. augmented reality, personal robotics or industrial automation. We introduce Complex-YOLO, a state of the art real-time 3D object detection network on point clouds only. In this work, we describe a network that expands YOLOv2, a fast 2D standard object detector for RGB images, by a specific complex regression strategy to estimate multi-class 3D boxes in Cartesian space. Thus, we propose a specific Euler-Region-Proposal Network (E-RPN) to estimate the pose of the object by adding an imaginary and a real fraction to the regression network. This ends up in a closed complex space and avoids singularities, which occur by single angle estimations. The E-RPN supports to generalize well during training. Our experiments on the KITTI benchmark suite show that we outperform current leading methods for 3D object detection specifically in terms of efficiency. We achieve state of the art results for cars, pedestrians and cyclists by being more than five times faster than the fastest competitor. Further, our model is capable of estimating all eight KITTI-classes, including Vans, Trucks or sitting pedestrians simultaneously with high accuracy.},
  file     = {:Simon_2018_ComplexYOLO.pdf:PDF},
  keywords = {YOLO, autonomous driving, 3D point clouds, object detection, deep Learning},
}

@Book{steger:2008:book,
  author    = {Carsten Steger and Markus Ulrich and Christian Wiedermann},
  title     = {Machine Vision Algorithms and Applications},
  year      = {2008},
  publisher = {Wiley-VCH},
  abstract  = {This first up-to-date textbook for machine vision software provides all the details on the theory and practical use of the relevant algorithms.The first part covers image acquisition, including illumination, lenses, cameras, frame grabbers, and bus systems, while the second deals with the algorithms themselves. This includes data structures, image enhancement and transformations, segmentation, feature extraction, morphology, template matching, stereo reconstruction, and camera calibration. The final part concentrates on applications, and features real-world examples, example code with HALCON, and further exercises.Uniting the latest research results with an industrial approach, this textbook is ideal for students of electrical engineering, physics and informatics, electrical and mechanical engineers, as well as those working in the sensor, automation and optical industries.},
  address   = {Weinheim, DE},
  file      = {:Steger_2008_MachineVisionAlgorithmsAndApplications.pdf:PDF},
  keywords  = {computer vision, image aquisition, image segmentation, feature extraction, camera calibration},
}

@Other{vicarious:2017:onl,
  Title                    = {Common Sense, Cortex, and CAPTCHA},
  Author                   = {vicarious},
  Date                     = {2017-10-26},
  Keywords                 = {machine learning, generative model, recursive cortical network, CAPTCHA, neuroscience},
  Url                      = {https://www.vicarious.com/2017/10/26/common-sense-cortex-and-captcha/},
  Year                     = {2017}
}

@Book{craig:2005:book,
  author    = {Craig, John J.},
  title     = {Introduction to Robotics: Mechanics and Control},
  year      = {2005},
  publisher = {Pearson Education},
  location  = {Upper Saddle River, US},
  file      = {:Craig_2005_IntroToRobotics.pdf:PDF},
  keywords  = {robotics, manipulator, kinematics, dynamics, trajectory generation, control, robot programming},
}

@Article{rumelhart:1986:art,
  author   = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  title    = {Learning representations by back-propagating errors},
  journal  = {Nature},
  year     = {1986},
  date     = {1986-10-09},
  volume   = {323},
  pages    = {533--536},
  url      = {http://dx.doi.org/10.1038/323533a0},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure.},
  file     = {:Rumelhart_1986_Backprop.pdf:PDF},
  keywords = {backpropagation, neural networks, optimization},
}

@InProceedings{sutskever:2013:proc,
  author    = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  title     = {On the Importance of Initialization and Momentum in Deep Learning},
  booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning},
  year      = {2013},
  volume    = {28},
  series    = {ICML'13},
  publisher = {JMLR},
  location  = {Atlanta, US},
  pages     = {III-1139--III-1147},
  abstract  = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods. },
  file      = {:Sutskever_2013_ImportanceOfInitializationAndMomentumInDeepLearning.pdf:PDF},
  keywords  = {deep learning, recurrent neural network, momentum update, parameter initialization},
}

@InProceedings{glorot:2010:proc,
    author = {Xavier Glorot and Yoshua Bengio},
    title = {Understanding the difficulty of training deep feedforward neural networks},
    booktitle = {In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS’10). Society for Artificial Intelligence and Statistics},
    year = {2010},
    volume    = {9},
    publisher = {JMLR},
    location  = {Chia Laguna Resort, IT},
    pages     = {249--256},
    abstract  = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
    file      = {:Glorot_2010_UnderstandingDifficultyOfTrainingDNNs.pdf:PDF},
    keywords  = {deep learning, initialization, gradient descent, optimization},
}

@Article{lecun:1998:art,
  author   = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
  title    = {Gradient-based learning applied to document recognition},
  journal  = {Proceedings of the IEEE},
  year     = {1998},
  volume   = {86},
  number   = {11},
  pages    = {2278--2324},
  issn     = {0018-9219},
  doi      = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day},
  file     = {:LeCun_1998_GradientBasedLearningAppliedToDocumentRecognition.pdf:PDF},
  keywords = {backpropagation;convolution;multilayer perceptrons;optical character recognition;2D shape variability;GTN;back-propagation;cheque reading;complex decision surface synthesis;convolutional neural network character recognizers;document recognition;document recognition systems;field extraction;gradient based learning technique;gradient-based learning;graph transformer networks;handwritten character recognition;handwritten digit recognition task;high-dimensional patterns;language modeling;multilayer neural networks;multimodule systems;performance measure minimization;segmentation recognition;Character recognition;Feature extraction;Hidden Markov models;Machine learning;Multi-layer neural network;Neural networks;Optical character recognition software;Optical computing;Pattern recognition;Principal component analysis, LeNet},
}

@Article{duchi:2011:art,
  author     = {Duchi, John and Hazan, Elad and Singer, Yoram},
  title      = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal    = {Journal of Machine Learning Research},
  year       = {2011},
  date       = {2011-07-11},
  volume     = {12},
  pages      = {2121--2159},
  issn       = {1532-4435},
  abstract   = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  file       = {:Duchi_2011_AdaptiveSubgradientMethods.pdf:PDF},
  issue_date = {2/1/2011},
  keywords   = {subgradient methods, adaptivity, online learning, stochastic convex optimization},
  publisher  = {JMLR},
}

@Article{zeiler:2012:art,
  author   = {Matthew D. Zeiler},
  title    = {{ADADELTA:} An Adaptive Learning Rate Method},
  journal  = {CoRR},
  year     = {2012},
  date     = {2012-12-22},
  volume   = {abs/1212.5701},
  eprint   = {1212.5701},
  url      = {http://arxiv.org/abs/1212.5701},
  abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
  file     = {:Zeiler_2012_ADAGRAD.pdf:PDF},
  keywords = {adaptive learning rates, ADADELTA, machine learning, neural networks, deep learning, gradient descent, optimization},
}

@Article{kingma:2014:art,
  author   = {Diederik P. Kingma and Jimmy Ba},
  title    = {Adam: {A} Method for Stochastic Optimization},
  journal  = {CoRR},
  year     = {2014},
  date     = {2014-12-22},
  volume   = {abs/1412.6980},
  eprint   = {1412.6980},
  url      = {http://arxiv.org/abs/1412.6980},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm. },
  file     = {:Kingma_2014_ADAM.pdf:PDF},
  keywords = {stochastic optimization, ADAM},
}

@Article{he:2015a:art,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on
               ImageNet Classification},
  journal   = {CoRR},
  year      = {2015},
  date     = {2014-02-06},
  volume    = {abs/1502.01852},
  eprint    = {1502.01852},
  url       = {http://arxiv.org/abs/1502.01852},
  Abstract  = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  file     = {:He_2015_DelvingDeepIntoRectifiers.pdf:PDF},
  keywords = {deep learning, activation function, parametric ReLU, robust initialization}
}

@Article{ioffe:2015:art,
  author    = {Sergey Ioffe and
               Christian Szegedy},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing
               Internal Covariate Shift},
  journal   = {CoRR},
  volume    = {abs/1502.03167},
  year      = {2015},
  date      = {2015-02-11},
  url       = {http://arxiv.org/abs/1502.03167},
  eprint    = {1502.03167},
  Abstract  = { Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters. },
  file      = {:Ioffe_2015_BatchNormalization.pdf:PDF},
  Keywords  = {deep learning, batch normalization, internal covariate shift, initialization, regularization}
}

@article{srivastava:2014:art,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html},
  Date    = {2014-07-15},
  abstract= {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  file  = {:Srivastava_2014_Dropout.pdf:PDF},
  keywords = {neural networks, deep learning, regularization, dropout, model combination}
  }

  @article{zeiler:2013:art,
    author    = {Matthew D. Zeiler and
                 Rob Fergus},
    title     = {Visualizing and Understanding Convolutional Networks},
    journal   = {CoRR},
    volume    = {abs/1311.2901},
    year      = {2013},
    url       = {http://arxiv.org/abs/1311.2901},
    eprint    = {1311.2901},
    Date      = {2013-11-12},
    Abstract  = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
    file  = {:Zeiler_2013_VisualizingAndUnderstandingCNNs.pdf:PDF},
    keywords = {deep learning, CNN, visualization, ImageNet}
  }

  @article{razavian:2014:art,
    author    = {Ali Sharif Razavian and Hossein Azizpour and Josephine Sullivan and Stefan Carlsson},
    title     = {{CNN} Features off-the-shelf: an Astounding Baseline for Recognition},
    journal   = {CoRR},
    volume    = {abs/1403.6382},
    year      = {2014},
    url       = {http://arxiv.org/abs/1403.6382},
    eprint    = {1403.6382},
    Date      = {2013-03-23},
    Abstract  = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
    file  = {:Razavian_2014_CNNFeaturesOffTheShelf.pdf:PDF},
    keywords = {deep learning, CNN, image classification, scene recognition, transfer learning}
  }

@inproceedings{krizhevsky:2012:proc,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems},
 series = {NIPS'12},
 year = {2012},
 volume = {1},
 location = {Lake Tahoe, US},
 pages = {1097--1105},
 numpages = {9},
 publisher = {Curran Associates Inc.},
 Abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
 file = {:Krizhevsky_2012_ImageNetClassificationWithDeepCNNs.pdf:PDF},
 keywords = {deep learning, CNNs, AlexNet, ImageNet}
}

@article{simonyan:2014:art,
  author    = {Karen Simonyan and
               Andrew Zisserman},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1409.1556},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.1556},
  eprint    = {1409.1556},
  date      = {2014-09-04},
  Abstract  = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  file = {:Simonyan_2014_VeryDeepCNNs.pdf:PDF},
  keywords = {deep learning, CNNs, VGG, image recognition}
}

@article{szegedy:2014:art,
  author    = {Christian Szegedy and
               Wei Liu and
               Yangqing Jia and
               Pierre Sermanet and
               Scott E. Reed and
               Dragomir Anguelov and
               Dumitru Erhan and
               Vincent Vanhoucke and
               Andrew Rabinovich},
  title     = {Going Deeper with Convolutions},
  journal   = {CoRR},
  volume    = {abs/1409.4842},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.4842},
  eprint    = {1409.4842},
  date      = {2014-09-17},
  Abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  file = {:Szegedy_2014_GoingDeeperWithConvolutions.pdf:PDF},
  keywords = {deep learning, CNNs, image recognition, GoogleNet, inception}
}

@article{he:2015b:art,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  eprint    = {1512.03385},
  date      = {2015-12-10},
  Abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \% COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  file = {:He_2015_DeepResidualLearningForImageRecognition.pdf:PDF},
  keywords = {deep learning, CNNs, image recognition, ResNet, residual layers}
}

@article{chollet:2016:art,
  author    = {Francois Chollet},
  title     = {Xception: Deep Learning with Depthwise Separable Convolutions},
  journal   = {CoRR},
  volume    = {abs/1610.02357},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.02357},
  eprint    = {1610.02357},
  Date      = {2016-10-07},
  Abstract  = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
  file = {:Chollet_2016_XceptionDeepLearningWithDepthwiseSeparableConvolutions.pdf:PDF},
  keywords = {deep learning, CNNs, Xception, inception, depthwise separable convolution, ImageNet}
}

@article{szegedy:2016:art,
  author    = {Christian Szegedy and
               Sergey Ioffe and
               Vincent Vanhoucke},
  title     = {Inception-v4, Inception-ResNet and the Impact of Residual Connections
               on Learning},
  journal   = {CoRR},
  volume    = {abs/1602.07261},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.07261},
  eprint    = {1602.07261},
  date      = {2016-02-26},
  Abstract  = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge },
  file = {:Szegedy_2016_InceptionV4InceptionResNetAndImpactOfResidualConnections.pdf:PDF},
  keywords = {deep learning, CNNs, inception, residual layers, image recognition}
}

@article{huang:2016:art,
  author    = {Gao Huang and
               Zhuang Liu and
               Kilian Q. Weinberger},
  title     = {Densely Connected Convolutional Networks},
  journal   = {CoRR},
  volume    = {abs/1608.06993},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.06993},
  eprint    = {1608.06993},
  Date      = {2016-08-25},
  Abstract  = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
  file = {:Huang_2016_DenselyConnectedConvolutionalNetworks.pdf:PDF},
  keywords = {deep learning, CNNs, dense layers, image recognition}
}

@article{toshev:2013:art,
  author    = {Alexander Toshev and
               Christian Szegedy},
  title     = {DeepPose: Human Pose Estimation via Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1312.4659},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.4659},
  eprint    = {1312.4659},
  date      = {2013-12-17},
  Abstract  = {We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regressors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formulation which capitalizes on recent advances in Deep Learning. We present a detailed empirical analysis with state-of-art or better performance on four academic benchmarks of diverse real-world images.},
  file = {:Toshev_2013_DeepPoseHumanPoseEstimation.pdf:PDF},
  keywords = {deep learning, CNNs, DeepPose, pose estimation, regression, localization}
}

@article{sermanet:2013:art,
  author    = {Pierre Sermanet and
               David Eigen and
               Xiang Zhang and
               Micha{\"{e}}l Mathieu and
               Rob Fergus and
               Yann LeCun},
  title     = {OverFeat: Integrated Recognition, Localization and Detection using
               Convolutional Networks},
  journal   = {CoRR},
  volume    = {abs/1312.6229},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.6229},
  eprint    = {1312.6229},
  date      = {2013-12-21},
  Abstract  = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
  file = {:Sermanet_2013_OverFeatIntegratedRecognitionLocalizationAndDetection.pdf:PDF},
  keywords = {deep learning, CNNs, OverFeat, object recognition, object localization, object detection}
}

@Article{uijlings:2013:art,
  author       = {Uijlings, J. R. R. and van de Sande, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
  title        = {Selective Search for Object Recognition},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  volume       = {104},
  pages        = {154--171},
  year         = {2013},
  url          = "https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013",
  abstract     = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 \% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available.},
  file = {:Uijlings_2013_SelectiveSearchForObjectRecognition.pdf:PDF},
  keywords = {object localization, selective search, exhaustive search, segmentation}
}

@article{ren:2015:art,
  author    = {Shaoqing Ren and
               Kaiming He and
               Ross B. Girshick and
               Jian Sun},
  title     = {Faster {R-CNN:} Towards Real-Time Object Detection with Region Proposal
               Networks},
  journal   = {CoRR},
  volume    = {abs/1506.01497},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.01497},
  eprint    = {1506.01497},
  Date      = {2015-06-04},
  Abstract  = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  file = {:Ren_2015_FasterRCNNTowardsRealTimeObjectDetection.pdf:PDF},
  keywords = {object detection, region proposal networks, R-CNN, deep learning, CNN}
}

@article{redmon:2015:art,
  author    = {Joseph Redmon and
               Santosh Kumar Divvala and
               Ross B. Girshick and
               Ali Farhadi},
  title     = {You Only Look Once: Unified, Real-Time Object Detection},
  journal   = {CoRR},
  volume    = {abs/1506.02640},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.02640},
  eprint    = {1506.02640},
  date      = {2015-06-08},
  abstract  = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  file = {:Redmon_2015_YOLOUnifiedRealTimeObjectDetection.pdf:PDF},
  keywords = {object detection, YOLO, deep learning, CNN}
}

@article{liu:2015:art,
  author    = {Wei Liu and
               Dragomir Anguelov and
               Dumitru Erhan and
               Christian Szegedy and
               Scott E. Reed and
               Cheng{-}Yang Fu and
               Alexander C. Berg},
  title     = {{SSD:} Single Shot MultiBox Detector},
  journal   = {CoRR},
  volume    = {abs/1512.02325},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.02325},
  eprint    = {1512.02325},
  date      = {2015-12-08},
  abstract  = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For 300x300 input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for 500×500 input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
  file = {:Liu_2015_SSDSingleShotMultiboxDetector.pdf:PDF},
  keywords = {object detection, SSD, deep learning, CNN}
}

@article{dai:2015:art,
  author    = {Jifeng Dai and
               Kaiming He and
               Jian Sun},
  title     = {Instance-aware Semantic Segmentation via Multi-task Network Cascades},
  journal   = {CoRR},
  volume    = {abs/1512.04412},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.04412},
  eprint    = {1512.04412},
  date      = {2015-12-14},
  abstract  = {Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multi-task Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems. The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place.},
  file = {:Dai_2015_InstanceAwareSemanticSegmentation.pdf:PDF},
  keywords = {deep learning, CNN, semantic segmentation, multi-task network cascades}
}

@article{long:2014:art,
  author    = {Jonathan Long and
               Evan Shelhamer and
               Trevor Darrell},
  title     = {Fully Convolutional Networks for Semantic Segmentation},
  journal   = {CoRR},
  volume    = {abs/1411.4038},
  year      = {2014},
  url       = {http://arxiv.org/abs/1411.4038},
  eprint    = {1411.4038},
  Date      = {2014-11-14},
  abstract  = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
  file = {:Long_2014_FullyConvNetsForSemanticSegmentation.pdf:PDF},
  keywords = {deep learning, CNN, fully convolutional layer, semantic segmentation}
}

@article{he:2017:art,
  author    = {Kaiming He and
               Georgia Gkioxari and
               Piotr Doll{\'{a}}r and
               Ross B. Girshick},
  title     = {Mask {R-CNN}},
  journal   = {CoRR},
  volume    = {abs/1703.06870},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.06870},
  eprint    = {1703.06870},
  Date      = {2017-03-20},
  Abstract  = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at https://github.com/facebookresearch/Detectron},
  file = {:He_2017_MaskRCNN.pdf:PDF},
  keywords = {deep learning, CNN, instance segmentation}
}

@article{dosovitskiy:2014:art,
  author    = {Alexey Dosovitskiy and
               Jost Tobias Springenberg and
               Thomas Brox},
  title     = {Learning to Generate Chairs with Convolutional Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1411.5928},
  year      = {2014},
  url       = {http://arxiv.org/abs/1411.5928},
  eprint    = {1411.5928},
  Date      = {2014-11-21},
  Abstract  = {We train generative 'up-convolutional' neural networks which are able to generate images of objects given object style, viewpoint, and color. We train the networks on rendered 3D models of chairs, tables, and cars. Our experiments show that the networks do not merely learn all images by heart, but rather find a meaningful representation of 3D models allowing them to assess the similarity of different models, interpolate between given views to generate the missing ones, extrapolate views, and invent new objects not present in the training set by recombining training instances, or even two different object classes. Moreover, we show that such generative networks can be used to find correspondences between different objects from the dataset, outperforming existing approaches on this task.},
  file = {:Dosovitskiy_2014_LearningToGenerateChairsWithCNNs.pdf:PDF},
  keywords = {deep learning, CNN, up-convolutional network, generative network, interpolation}
}

@incollection{goodfellow:2014:col,
title = {Generative Adversarial Nets},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {2672--2680},
year = {2014},
Date  = {2014-06-10},
publisher = {Curran Associates, Inc.},
abstract  = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
file = {:Goodfellow_2014_GenerativeAdversarialNets.pdf:PDF},
keywords = {deep learning, CNN, GAN, generative models, adversiarial process, discriminative model}
}

@article{zhu:2017:art,
  author    = {Jun{-}Yan Zhu and
               Taesung Park and
               Phillip Isola and
               Alexei A. Efros},
  title     = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial
               Networks},
  journal   = {CoRR},
  volume    = {abs/1703.10593},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.10593},
  eprint    = {1703.10593},
  date      = {2017-03-30},
  abstract  = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G:X->Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F:Y->X and introduce a cycle consistency loss to push F(G(X))~X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  file = {:Zhu_2017_UnpairedImageToImageTranslationUsingCycleConsistentGANs.pdf:PDF},
  keywords = {deep learning, CNN, GAN, generative models, Image-to-image translation, cycle consistency},
}

@article{karras:2017:art,
  author    = {Tero Karras and
               Timo Aila and
               Samuli Laine and
               Jaakko Lehtinen},
  title     = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  journal   = {CoRR},
  volume    = {abs/1710.10196},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.10196},
  eprint    = {1710.10196},
  Date      = {2017-10-27},
  Abstract  = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
  file = {:Karras_2017_ProgressiveGrowingOfGANs.pdf:PDF},
  keywords = {deep learning, CNN, progressive GAN, generative models},
}

@article{oord:2016a:art,
  author    = {Aäron van den Oord and
               Nal Kalchbrenner and
               Oriol Vinyals and
               Lasse Espeholt and
               Alex Graves and
               Koray Kavukcuoglu},
  title     = {Conditional Image Generation with PixelCNN Decoders},
  journal   = {CoRR},
  volume    = {abs/1606.05328},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.05328},
  eprint    = {1606.05328},
  Date      = {2016-06-16},
  Abstract  = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
  file = {:Oord_2016_ConditionalImageGenerationWithPixelCNN.pdf:PDF},
  keywords = {deep learning, CNN, generative models, PixelCNN, conditional image generation},
}

@article{oord:2016b:art,
  author    = {Aäron van den Oord and
               Nal Kalchbrenner and
               Koray Kavukcuoglu},
  title     = {Pixel Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1601.06759},
  year      = {2016},
  url       = {http://arxiv.org/abs/1601.06759},
  eprint    = {1601.06759},
  Date      = {2016-01-25},
  Abstract  = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
  file = {:Oord_2016_PixelRecurrentNeuralNetworks.pdf:PDF},
  keywords = {deep learning, CNN, generative models, PixelCNN, sequentially predictions},
}

@article{reed:2017a:art,
  author    = {Scott E. Reed and
               Aäron van den Oord and
               Nal Kalchbrenner and
               Sergio Gomez Colmenarejo and
               Ziyu Wang and
               Dan Belov and
               Nando de Freitas},
  title     = {Parallel Multiscale Autoregressive Density Estimation},
  journal   = {CoRR},
  volume    = {abs/1703.03664},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.03664},
  eprint    = {1703.03664},
  Date      = {2017-03-10},
  Abstract  = {PixelCNN achieves state-of-the-art results in density estimation for natural images. Although training is fast, inference is costly, requiring one network evaluation per pixel; O(N) for N pixels. This can be sped up by caching activations, but still involves generating each pixel sequentially. In this work, we propose a parallelized PixelCNN that allows more efficient inference by modeling certain pixel groups as conditionally independent. Our new PixelCNN model achieves competitive density estimation and orders of magnitude speedup - O(log N) sampling instead of O(N) - enabling the practical generation of 512x512 images. We evaluate the model on class-conditional image generation, text-to-image synthesis, and action-conditional video generation, showing that our model achieves the best results among non-pixel-autoregressive density models that allow efficient sampling.},
  file = {:Reed_2017_ParallelMultiscaleAutoregressiveDensityEstimation.pdf:PDF},
  keywords = {deep learning, CNN, generative models, autoregressive density estimation},
}

@article{oord:2016c:art,
  author    = {Aäron van den Oord and
               Sander Dieleman and
               Heiga Zen and
               Karen Simonyan and
               Oriol Vinyals and
               Alex Graves and
               Nal Kalchbrenner and
               Andrew W. Senior and
               Koray Kavukcuoglu},
  title     = {WaveNet: {A} Generative Model for Raw Audio},
  journal   = {CoRR},
  volume    = {abs/1609.03499},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.03499},
  eprint    = {1609.03499},
  Date      = {2016-09-12},
  Abstract  = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  file = {:Oord_2016_WaveNetAGenerativeModelForRawAudio.pdf:PDF},
  keywords = {deep learning, CNN, generative models, WaveNet, text-to-speech},
}

@article{hochreiter:1997:art,
 author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
 title = {Long Short-Term Memory},
 journal = {Neural Comput.},
 issue_date = {November 15, 1997},
 volume = {9},
 number = {8},
 year = {1997},
 issn = {0899-7667},
 pages = {1735--1780},
 url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
 doi = {10.1162/neco.1997.9.8.1735},
 publisher = {MIT Press},
 address = {Cambridge, US},
 Abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
 file = {:Hochreiter_1997_LongShortTermMemory.pdf:PDF},
 keywords = {machine learning, LSTM, RNN, neural networks},
}

@article{xu:2015:art,
  author    = {Kelvin Xu and
               Jimmy Ba and
               Ryan Kiros and
               Kyunghyun Cho and
               Aaron C. Courville and
               Ruslan Salakhutdinov and
               Richard S. Zemel and
               Yoshua Bengio},
  title     = {Show, Attend and Tell: Neural Image Caption Generation with Visual
               Attention},
  journal   = {CoRR},
  volume    = {abs/1502.03044},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.03044},
  eprint    = {1502.03044},
  date      = {2015-02-10},
  Abstract  = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
  file = {:Xu_2015_ShowAttendTellNeuralImageCaptionGeneration.pdf:PDF},
  keywords = {deep learning, CNN, RNN, image captioning, visual attention},
}

@article{walch:2016:art,
  author    = {Florian Walch and
               Caner Hazirbas and
               Laura Leal{-}Taix{\'{e}} and
               Torsten Sattler and
               Sebastian Hilsenbeck and
               Daniel Cremers},
  title     = {Image-based Localization with Spatial LSTMs},
  journal   = {CoRR},
  volume    = {abs/1611.07890},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.07890},
  eprint    = {1611.07890},
  Date      = {2016-11-23},
  Abstract  = {In this work we propose a new CNN+LSTM architecture for camera pose regression for indoor and outdoor scenes. CNNs allow us to learn suitable feature representations for localization that are robust against motion blur and illumination changes. We make use of LSTM units on the CNN output, which play the role of a structured dimensionality reduction on the feature vector, leading to drastic improvements in localization performance. We provide extensive quantitative comparison of CNN-based and SIFT-based localization methods, showing the weaknesses and strengths of each. Furthermore, we present a new large-scale indoor dataset with accurate ground truth from a laser scanner. Experimental results on both indoor and outdoor public datasets show our method outperforms existing deep architectures, and can localize images in hard conditions, e.g., in the presence of mostly textureless surfaces, where classic SIFT-based methods fail.},
  file = {:Walch_2016_ImageBasedLocalizationWithSpatialLSTMs.pdf:PDF},
  keywords = {deep learning, CNN, RNN, LSTM, image-based localization},
}

@article{gehring:2017:art,
  author    = {Jonas Gehring and
               Michael Auli and
               David Grangier and
               Denis Yarats and
               Yann N. Dauphin},
  title     = {Convolutional Sequence to Sequence Learning},
  journal   = {CoRR},
  volume    = {abs/1705.03122},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.03122},
  eprint    = {1705.03122},
  Date      = {2017-06-08},
  Abstract  = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.},
  file = {:Gehring_2017_ConvolutionalSequenceToSequenceLearning.pdf:PDF},
  keywords = {deep learning, CNN, RNN, gated linear units, sequence to sequence},
}


@article{vaswani:2017:art,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  eprint    = {1706.03762},
  date      = {2017-07-12},
  Abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. },
  file = {:Vaswani_2017_AttentionIsAllYouNeed.pdf:PDF},
  keywords = {deep learning, CNN, RNN, transformer, attention mechanism, language translation},
}

@article{dai:2017:art,
  author    = {Angela Dai and
               Daniel Ritchie and
               Martin Bokeloh and
               Scott Reed and
               Jürgen Sturm and
               Matthias Nießner},
  title     = {ScanComplete: Large-Scale Scene Completion and Semantic Segmentation
               for 3D Scans},
  journal   = {CoRR},
  volume    = {abs/1712.10215},
  url       = {http://arxiv.org/abs/1712.10215},
  eprint    = {1712.10215},
  year      = {2017},
  date      = {2017-12-29},
  Abstract  = {We introduce ScanComplete, a novel data-driven approach for taking an incomplete 3D scan of a scene as input and predicting a complete 3D model along with per-voxel semantic labels. The key contribution of our method is its ability to handle large scenes with varying spatial extent, managing the cubic growth in data size as scene size increases. To this end, we devise a fully-convolutional generative 3D CNN model whose filter kernels are invariant to the overall scene size. The model can be trained on scene subvolumes but deployed on arbitrarily large scenes at test time. In addition, we propose a coarse-to-fine inference strategy in order to produce high-resolution output while also leveraging large input context sizes. In an extensive series of experiments, we carefully evaluate different model design choices, considering both deterministic and probabilistic models for completion and semantic inference. Our results show that we outperform other methods not only in the size of the environments handled and processing efficiency, but also with regard to completion quality and semantic segmentation performance by a significant margin.},
  file = {:Dai_2017_ScanCompleteLargeScaleSceneCompletion.pdf:PDF},
  keywords = {deep learning, CNN, ScanComplete, scene completion, 3D scans, semantic segmentation},
}

@article{reed:2017b:art,
  author    = {Scott E. Reed and
               Yutian Chen and
               Thomas Paine and
               Aäron van den Oord and
               S. M. Ali Eslami and
               Danilo Jimenez Rezende and
               Oriol Vinyals and
               Nando de Freitas},
  title     = {Few-shot Autoregressive Density Estimation: Towards Learning to Learn
               Distributions},
  journal   = {CoRR},
  volume    = {abs/1710.10304},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.10304},
  eprint    = {1710.10304},
  date      = {2017-10-27},
  Abstract  = {Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet. However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks. In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset. Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.},
  file = {:Reed_2017_FesSHotAutoregressiveDensityEstimation.pdf:PDF},
  keywords = {deep learning, CNN, autoregressive models, neural attention, meta learning, PixelCNN, few-shot},
}

@article{kalchbrenner:2016:art,
  author    = {Nal Kalchbrenner and
               Aäron van den Oord and
               Karen Simonyan and
               Ivo Danihelka and
               Oriol Vinyals and
               Alex Graves and
               Koray Kavukcuoglu},
  title     = {Video Pixel Networks},
  journal   = {CoRR},
  volume    = {abs/1610.00527},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.00527},
  eprint    = {1610.00527},
  Date      = {2017-10-03},
  Abstract  = {We propose a probabilistic video model, the Video Pixel Network (VPN), that estimates the discrete joint distribution of the raw pixel values in a video. The model and the neural architecture reflect the time, space and color structure of video tensors and encode it as a four-dimensional dependency chain. The VPN approaches the best possible performance on the Moving MNIST benchmark, a leap over the previous state of the art, and the generated videos show only minor deviations from the ground truth. The VPN also produces detailed samples on the action-conditional Robotic Pushing benchmark and generalizes to the motion of novel objects.},
  file = {:Kalchbrenner_2016_VideoPixelNetworks.pdf:PDF},
  keywords = {deep learning, CNN, video pixel network, video modelling},
}

@article{babaeizadeh:2017:art,
  author    = {Mohammad Babaeizadeh and
               Chelsea Finn and
               Dumitru Erhan and
               Roy H. Campbell and
               Sergey Levine},
  title     = {Stochastic Variational Video Prediction},
  journal   = {CoRR},
  volume    = {abs/1710.11252},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.11252},
  eprint    = {1710.11252},
  date      = {2017-10-30},
  Abstract  = {Predicting the future in real-world settings, particularly from raw sensory observations such as images, is exceptionally challenging. Real-world events can be stochastic and unpredictable, and the high dimensionality and complexity of natural images requires the predictive model to build an intricate understanding of the natural world. Many existing methods tackle this problem by making simplifying assumptions about the environment. One common assumption is that the outcome is deterministic and there is only one plausible future. This can lead to low-quality predictions in real-world settings with stochastic dynamics. In this paper, we develop a stochastic variational video prediction (SV2P) method that predicts a different possible future for each sample of its latent variables. To the best of our knowledge, our model is the first to provide effective stochastic multi-frame prediction for real-world video. We demonstrate the capability of the proposed method in predicting detailed future frames of videos on multiple real-world datasets, both action-free and action-conditioned. We find that our proposed method produces substantially improved video predictions when compared to the same model without stochasticity, and to other stochastic video prediction methods. Our SV2P implementation will be open sourced upon publication.},
  file = {:Babaeizadeh_2017_StochasticVariationalVideoPrediction.pdf:PDF},
  keywords = {deep learning, CNN, stochastic variational video prediction, future prediction},
}

@article{villegas:2017:art,
  author    = {Ruben Villegas and
               Jimei Yang and
               Yuliang Zou and
               Sungryull Sohn and
               Xunyu Lin and
               Honglak Lee},
  title     = {Learning to Generate Long-term Future via Hierarchical Prediction},
  journal   = {CoRR},
  volume    = {abs/1704.05831},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.05831},
  eprint    = {1704.05831},
  date      = {2017-04-19},
  Abstract  = {We propose a hierarchical approach for making long-term predictions of future frames. To avoid inherent compounding errors in recursive pixel-level prediction, we propose to first estimate high-level structure in the input frames, then predict how that structure evolves in the future, and finally by observing a single frame from the past and the predicted high-level structure, we construct the future frames without having to observe any of the pixel-level predictions. Long-term video prediction is difficult to perform by recurrently observing the predicted frames because the small errors in pixel space exponentially amplify as predictions are made deeper into the future. Our approach prevents pixel-level error propagation from happening by removing the need to observe the predicted frames. Our model is built with a combination of LSTM and analogy based encoder-decoder convolutional neural networks, which independently predict the video structure and generate the future frames, respectively. In experiments, our model is evaluated on the Human3.6M and Penn Action datasets on the task of long-term pixel-level video prediction of humans performing actions and demonstrate significantly better results than the state-of-the-art.},
  file = {:Villegas_2017_LearningToGenerateLongTermFutureHierarchicalPrediction.pdf:PDF},
  keywords = {deep learning, CNN, long-term prediction, hierarchical prediction},
}

@article{bellemare:2016:art,
  author    = {Marc G. Bellemare and
               Sriram Srinivasan and
               Georg Ostrovski and
               Tom Schaul and
               David Saxton and
               R{\'{e}}mi Munos},
  title     = {Unifying Count-Based Exploration and Intrinsic Motivation},
  journal   = {CoRR},
  volume    = {abs/1606.01868},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.01868},
  eprint    = {1606.01868},
  date      = {2016-06-06},
  Abstract  = {We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.},
  file = {:Bellemare_2016_UnifyingCountBasedExplorationAndIntrinsicMotivation.pdf:PDF},
  keywords = {deep learning, reinforcement learning, exploration algorithms, uncertainty},
}

@article{ostrovski:2017:art,
  author    = {Georg Ostrovski and
               Marc G. Bellemare and
               A{\"{a}}ron van den Oord and
               R{\'{e}}mi Munos},
  title     = {Count-Based Exploration with Neural Density Models},
  journal   = {CoRR},
  volume    = {abs/1703.01310},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.01310},
  eprint    = {1703.01310},
  date      = {2017-03-03},
  Abstract  = { Bellemare et al. (2016) introduced the notion of a pseudo-count, derived from a density model, to generalize count-based exploration to non-tabular reinforcement learning. This pseudo-count was used to generate an exploration bonus for a DQN agent and combined with a mixed Monte Carlo update was sufficient to achieve state of the art on the Atari 2600 game Montezuma's Revenge. We consider two questions left open by their work: First, how important is the quality of the density model for exploration? Second, what role does the Monte Carlo update play in exploration? We answer the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to supply a pseudo-count. In particular, we examine the intrinsic difficulties in adapting Bellemare et al.'s approach when assumptions about the model are violated. The result is a more practical and general algorithm requiring no special apparatus. We combine PixelCNN pseudo-counts with different agent architectures to dramatically improve the state of the art on several hard Atari games. One surprising finding is that the mixed Monte Carlo update is a powerful facilitator of exploration in the sparsest of settings, including Montezuma's Revenge.},
  file = {:Bellemare_2016_UnifyingCountBasedExplorationAndIntrinsicMotivation.pdf:PDF},
  keywords = {deep learning, reinforcement learning, exploration algorithms, uncertainty, PixelCNN, Monte Carlo update},
}

@article{weber:2017:art,
  author    = {Theophane Weber and
               S{\'{e}}bastien Racani{\`{e}}re and
               David P. Reichert and
               Lars Buesing and
               Arthur Guez and
               Danilo Jimenez Rezende and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Oriol Vinyals and
               Nicolas Heess and
               Yujia Li and
               Razvan Pascanu and
               Peter Battaglia and
               David Silver and
               Daan Wierstra},
  title     = {Imagination-Augmented Agents for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1707.06203},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06203},
  eprint    = {1707.06203},
  date      = {2017-07-19},
  Abstract  = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
  file     = {:Weber_2017_ImaginationAugmentedAgentsForDeepReinforcementLearning.pdf:PDF},
  keywords = {deep learning, CNN, imagination-augmented agents, reinfocement learning, planning methods},
}

@article{bousmalis:2017:art,
  author    = {Konstantinos Bousmalis and
               Alex Irpan and
               Paul Wohlhart and
               Yunfei Bai and
               Matthew Kelcey and
               Mrinal Kalakrishnan and
               Laura Downs and
               Julian Ibarz and
               Peter Pastor and
               Kurt Konolige and
               Sergey Levine and
               Vincent Vanhoucke},
  title     = {Using Simulation and Domain Adaptation to Improve Efficiency of Deep
               Robotic Grasping},
  journal   = {CoRR},
  volume    = {abs/1709.07857},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.07857},
  eprint    = {1709.07857},
  date      = {2017-09-22},
  Abstract  = {Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the GraspGAN. We show that, by using synthetic data and domain adaptation, we are able to reduce the number of real-world samples needed to achieve a given level of performance by up to 50 times, using only randomly generated simulated objects. We also show that by using only unlabeled real-world data and our GraspGAN methodology, we obtain real-world grasping performance without any real-world labels that is similar to that achieved with 939,777 labeled real-world samples.},
  file     = {:Bousmalis_2017_UsingSimulationAndDomainAdaptionForDeepRoboticGrasping.pdf:PDF},
  keywords = {deep learning, robotic grasping, GraspGAN, domain adaption, simulation},
}

@Comment{jabref-meta: databaseType:biblatex;}
